{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Face Style Sketches Creator Using JojoGAN**"
      ],
      "metadata": {
        "id": "g4xf-NXsCmxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will employ and train the JojoGAN model to generate face style sketches by finetunning the StyleGAN model."
      ],
      "metadata": {
        "id": "J3jLEkd2Cw7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clone JojoGAN Repo And Install Required Libraries**"
      ],
      "metadata": {
        "id": "s4FhxCz6Ae8O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8T4JWQPAOIy",
        "outputId": "06896a50-1cbb-4fd5-ab1c-93d5fb3493a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'JoJoGAN'...\n",
            "remote: Enumerating objects: 490, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 490 (delta 31), reused 31 (delta 31), pack-reused 450\u001b[K\n",
            "Receiving objects: 100% (490/490), 63.51 MiB | 11.81 MiB/s, done.\n",
            "Resolving deltas: 100% (203/203), done.\n",
            "/content/JoJoGAN\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Collecting scikit-learn==0.22\n",
            "  Downloading scikit-learn-0.22.tar.gz (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Collecting lpips\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dlib in /usr/local/lib/python3.10/dist-packages (19.24.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.22) (1.23.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==0.22) (1.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from lpips) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (0.16.0+cu121)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.39.1-py2.py3-none-any.whl (254 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.11.17)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.2.1->lpips) (9.4.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->lpips) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.0->lpips) (1.3.0)\n",
            "Building wheels for collected packages: scikit-learn\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for scikit-learn (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for scikit-learn\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for scikit-learn\n",
            "Failed to build scikit-learn\n",
            "\u001b[31mERROR: Could not build wheels for scikit-learn, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m--2024-01-08 07:37:59--  https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240108%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240108T073557Z&X-Amz-Expires=300&X-Amz-Signature=0b5f5539770a8f0973118db12162f8222042656bd902ce056479c836b6d57e4e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-01-08 07:37:59--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240108%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240108T073557Z&X-Amz-Expires=300&X-Amz-Signature=0b5f5539770a8f0973118db12162f8222042656bd902ce056479c836b6d57e4e&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77854 (76K) [application/octet-stream]\n",
            "Saving to: ‘ninja-linux.zip’\n",
            "\n",
            "ninja-linux.zip     100%[===================>]  76.03K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-01-08 07:38:01 (56.2 MB/s) - ‘ninja-linux.zip’ saved [77854/77854]\n",
            "\n",
            "Archive:  ninja-linux.zip\n",
            "  inflating: /usr/local/bin/ninja    \n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mchong6/JoJoGAN.git\n",
        "%cd JoJoGAN\n",
        "!pip install tqdm gdown scikit-learn==0.22 scipy lpips dlib opencv-python wandb\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ba4L6bPaBHF4",
        "outputId": "d8ad2ada-8ce9-45f9-8d1a-ec2d3469eb4d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Using cached wandb-0.16.1-py3-none-any.whl (2.1 MB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Using cached GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Using cached sentry_sdk-1.39.1-py2.py3-none-any.whl (254 kB)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Using cached setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.39.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "r1F-LUHLKCZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.backends.cudnn.benchmark = True\n",
        "from torchvision import transforms, utils\n",
        "from util import *\n",
        "from PIL import Image\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "import numpy\n",
        "from torch import nn, autograd, optim\n",
        "from torch.nn import functional\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "from model import *\n",
        "from e4e_projection import projection\n",
        "from google.colab import files\n",
        "from copy import deepcopy\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOjD90hfApYi",
        "outputId": "60a017ea-41b1-4090-bea8-11d716d1fe61"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Storage Folders**"
      ],
      "metadata": {
        "id": "gJo3KRVLKF_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('inversion_codes', exist_ok=True)\n",
        "os.makedirs('style_images', exist_ok=True)\n",
        "os.makedirs('style_images_aligned', exist_ok=True)\n",
        "os.makedirs('models', exist_ok=True)"
      ],
      "metadata": {
        "id": "03Om_c-mBgj9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download Pydrive**"
      ],
      "metadata": {
        "id": "66IJ-KKQKQt9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the model files with Pydrive."
      ],
      "metadata": {
        "id": "fjE_wn1kNmLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "download_with_pydrive = True\n",
        "device = 'cuda' #['cuda', 'cpu']"
      ],
      "metadata": {
        "id": "jZHOTyLABuw4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2\n",
        "!mv shape_predictor_68_face_landmarks.dat models/dlibshape_predictor_68_face_landmarks.dat\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dChk0mEBwTz",
        "outputId": "9019e5d1-5b66-40b6-f49d-01a039732e46"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-08 07:44:03--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
            "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64040097 (61M)\n",
            "Saving to: ‘shape_predictor_68_face_landmarks.dat.bz2’\n",
            "\n",
            "shape_predictor_68_ 100%[===================>]  61.07M  10.5MB/s    in 14s     \n",
            "\n",
            "2024-01-08 07:44:17 (4.25 MB/s) - ‘shape_predictor_68_face_landmarks.dat.bz2’ saved [64040097/64040097]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Drive IDs**"
      ],
      "metadata": {
        "id": "iVZOaw2TNz9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive_ids = {\n",
        "    \"stylegan2-ffhq-config-f.pt\": \"1Yr7KuD959btpmcKGAUsbAk5rPjX2MytK\",\n",
        "    \"e4e_ffhq_encode.pt\": \"1o6ijA3PkcewZvwJJ73dJ0fxhndn0nnh7\",\n",
        "    \"restyle_psp_ffhq_encode.pt\": \"1nbxCIVw9H3YnQsoIPykNEFwWJnHVHlVd\",\n",
        "    \"arcane_caitlyn.pt\": \"1gOsDTiTPcENiFOrhmkkxJcTURykW1dRc\",\n",
        "    \"arcane_caitlyn_preserve_color.pt\": \"1cUTyjU-q98P75a8THCaO545RTwpVV-aH\",\n",
        "    \"arcane_jinx_preserve_color.pt\": \"1jElwHxaYPod5Itdy18izJk49K1nl4ney\",\n",
        "    \"arcane_jinx.pt\": \"1quQ8vPjYpUiXM4k1_KIwP4EccOefPpG_\",\n",
        "    \"arcane_multi_preserve_color.pt\": \"1enJgrC08NpWpx2XGBmLt1laimjpGCyfl\",\n",
        "    \"arcane_multi.pt\": \"15V9s09sgaw-zhKp116VHigf5FowAy43f\",\n",
        "    \"sketch_multi.pt\": \"1GdaeHGBGjBAFsWipTL0y-ssUiAqk8AxD\",\n",
        "    \"disney.pt\": \"1zbE2upakFUAx8ximYnLofFwfT8MilqJA\",\n",
        "    \"disney_preserve_color.pt\": \"1Bnh02DjfvN_Wm8c4JdOiNV4q9J7Z_tsi\",\n",
        "    \"jojo.pt\": \"13cR2xjIBj8Ga5jMO7gtxzIJj2PDsBYK4\",\n",
        "    \"jojo_preserve_color.pt\": \"1ZRwYLRytCEKi__eT2Zxv1IlV6BGVQ_K2\",\n",
        "    \"jojo_yasuho.pt\": \"1grZT3Gz1DLzFoJchAmoj3LoM9ew9ROX_\",\n",
        "    \"jojo_yasuho_preserve_color.pt\": \"1SKBu1h0iRNyeKBnya_3BBmLr4pkPeg_L\",\n",
        "    \"art.pt\": \"1a0QDEHwXQ6hE_FcYEyNMuv5r5UnRQLKT\",\n",
        "}"
      ],
      "metadata": {
        "id": "1PuWrGvLB74R"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Downloader Class**"
      ],
      "metadata": {
        "id": "oTEkHTTnN6LF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a class to download files. This implementation is from StyleGAN-NADA."
      ],
      "metadata": {
        "id": "6_WVjnfSOLAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Downloader(object):\n",
        "    def __init__(self, use_pydrive):\n",
        "        self.use_pydrive = use_pydrive\n",
        "\n",
        "\n",
        "        if self.use_pydrive:\n",
        "            self.authenticate()\n",
        "    def authenticate(self):\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        self.drive = GoogleDrive(gauth)\n",
        "    def download_file(self, file_name):\n",
        "        file_dst = os.path.join('models', file_name)\n",
        "        file_id = drive_ids[file_name]\n",
        "        if not os.path.exists(file_dst):\n",
        "            print(f'Downloading {file_name}')\n",
        "            if self.use_pydrive:\n",
        "                downloaded = self.drive.CreateFile({'id':file_id})\n",
        "                downloaded.FetchMetadata(fetch_all=True)\n",
        "                downloaded.GetContentFile(file_dst)\n",
        "            else:\n",
        "                !gdown --id $file_id -O $file_dst"
      ],
      "metadata": {
        "id": "9LUgHOEoB9y3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download Files**"
      ],
      "metadata": {
        "id": "ocAe4t7GOsOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "downloader = Downloader(download_with_pydrive)\n",
        "\n",
        "downloader.download_file('stylegan2-ffhq-config-f.pt')\n",
        "downloader.download_file('e4e_ffhq_encode.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6VFcvIJCQNc",
        "outputId": "edff370a-892a-4e02-a0f3-b2161ce4b926"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading stylegan2-ffhq-config-f.pt\n",
            "Downloading e4e_ffhq_encode.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Generators**"
      ],
      "metadata": {
        "id": "aHg_UDM7O41N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the original and finetuned generators."
      ],
      "metadata": {
        "id": "oRQNbVCUO8kq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 512\n",
        "\n",
        "# Load original generator\n",
        "original_generator = Generator(1024, latent_dim, 8, 2).to(device)\n",
        "\n",
        "ckpt = torch.load('models/stylegan2-ffhq-config-f.pt', map_location=lambda storage, loc: storage)\n",
        "\n",
        "original_generator.load_state_dict(ckpt[\"g_ema\"], strict=False)\n",
        "\n",
        "mean_latent = original_generator.mean_latent(10000)\n",
        "\n",
        "# to be finetuned generator\n",
        "generator = deepcopy(original_generator)"
      ],
      "metadata": {
        "id": "XLSEDZbLDddP"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Tranformer**"
      ],
      "metadata": {
        "id": "muAm3tHRPWgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the transformer for resizing and normalizing the images."
      ],
      "metadata": {
        "id": "z-thz90rPcv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((1024, 1024)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "aY_IofLTDrIM"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Input Image Location**"
      ],
      "metadata": {
        "id": "ETPlAJpSPlMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'iu.jpeg' #@param {type:\"string\"}\n",
        "filepath = f'test_input/{filename}'\n",
        "name = strip_path_extension(filepath)+'.pt'"
      ],
      "metadata": {
        "id": "K_nJTBeZDujQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Align And Crop Face**"
      ],
      "metadata": {
        "id": "8M1DeDx5PrYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aligned_face = align_face(filepath)"
      ],
      "metadata": {
        "id": "Vc7sGWJ-D2yN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restyle The Projection**"
      ],
      "metadata": {
        "id": "daPuXZPwPzti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# my_w = restyle_projection(aligned_face, name, device, n_iters=1).unsqueeze(0)\n",
        "my_w = projection(aligned_face, name, device).unsqueeze(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XgR39zKEeyW",
        "outputId": "b5ff6772-ab33-4805-ab5f-a7140bd5ae23"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading e4e over the pSp framework from checkpoint: models/e4e_ffhq_encode.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Select Pre-Trained Sketch Type**"
      ],
      "metadata": {
        "id": "xjT3ekf6QAVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.dpi'] = 150\n",
        "pretrained = 'sketch_multi' #['art', 'arcane_multi', 'sketch_multi', 'arcane_jinx', 'arcane_caitlyn', 'jojo_yasuho', 'jojo', 'disney']"
      ],
      "metadata": {
        "id": "J8AJ6VPVEqdK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate Results**"
      ],
      "metadata": {
        "id": "Ny97G72VQZ44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the checkpoint and generator,set a seed, and start generating a stylized image."
      ],
      "metadata": {
        "id": "eJigCWaoQiIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_sample =  5#{type:\"number\"}\n",
        "seed = 3000 #{type:\"number\"}\n",
        "torch.manual_seed(seed)\n",
        "with torch.no_grad():\n",
        "    generator.eval()\n",
        "    z = torch.randn(n_sample, latent_dim, device=device)\n",
        "    original_sample = original_generator([z], truncation=0.7, truncation_latent=mean_latent)\n",
        "    sample = generator([z], truncation=0.7, truncation_latent=mean_latent)\n",
        "    original_my_sample = original_generator(my_w, input_is_latent=True)\n",
        "    my_sample = generator(my_w, input_is_latent=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYJg4QD3Gh1b",
        "outputId": "8e789e2e-bda7-4668-8a51-794698e2574e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/JoJoGAN/op/conv2d_gradfix.py:88: UserWarning: conv2d_gradfix not supported on PyTorch 2.1.0+cu121. Falling back to torch.nn.functional.conv2d().\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display Reference Images**"
      ],
      "metadata": {
        "id": "_tKI5zhmQ0Gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if pretrained == 'arcane_multi':\n",
        "    style_path = f'style_images_aligned/arcane_jinx.png'\n",
        "elif pretrained == 'sketch_multi':\n",
        "    style_path = f'style_images_aligned/sketch.png'\n",
        "else:\n",
        "    style_path = f'style_images_aligned/{pretrained}.png'\n",
        "style_image = transform(Image.open(style_path)).unsqueeze(0).to(device)\n",
        "face = transform(aligned_face).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "my_output = torch.cat([style_image, face, my_sample], 0)"
      ],
      "metadata": {
        "id": "0KAUnknJGoPj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Model With Style Images**"
      ],
      "metadata": {
        "id": "okCbxQdJQ4QU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select Face Sketch Images and load them to train the model."
      ],
      "metadata": {
        "id": "Awk_sUGLRX50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "names = ['sketch.jpeg', 'sketch2.jpeg', 'sketch3.jpeg']\n",
        "targets = []\n",
        "latents = []\n",
        "for name in names:\n",
        "    style_path = os.path.join('style_images', name)\n",
        "    assert os.path.exists(style_path), f\"{style_path} does not exist!\"\n",
        "    name = strip_path_extension(name)\n",
        "\n",
        "\n",
        "    # crop and align the face\n",
        "    style_aligned_path = os.path.join('style_images_aligned', f'{name}.png')\n",
        "    if not os.path.exists(style_aligned_path):\n",
        "        style_aligned = align_face(style_path)\n",
        "        style_aligned.save(style_aligned_path)\n",
        "    else:\n",
        "        style_aligned = Image.open(style_aligned_path).convert('RGB')\n",
        "\n",
        "\n",
        "    # GAN invert\n",
        "    style_code_path = os.path.join('inversion_codes', f'{name}.pt')\n",
        "    if not os.path.exists(style_code_path):\n",
        "        latent = projection(style_aligned, style_code_path, device)\n",
        "    else:\n",
        "        latent = torch.load(style_code_path)['latent']\n",
        "    latents.append(latent.to(device))\n",
        "#targets = torch.stack(targets, 0)\n",
        "latents = torch.stack(latents, 0)"
      ],
      "metadata": {
        "id": "B2iC92LrG7vd"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finetune StyleGAN**"
      ],
      "metadata": {
        "id": "ntsHgfpTSlSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set alpha which controls the strength of the style."
      ],
      "metadata": {
        "id": "lCj0ssFOSoKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha =  1.0 # min:0, max:1, step:0.1\n",
        "alpha = 1-alpha"
      ],
      "metadata": {
        "id": "V-O_m4i6HyNf"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preserve color of original image by limiting family of allowable transformations."
      ],
      "metadata": {
        "id": "ZEIlSdzYS9Kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preserve_color = False\n",
        "#Number of finetuning steps.\n",
        "num_iter = 300\n",
        "#Log training on wandb and interval for image logging\n",
        "use_wandb = False\n",
        "log_interval = 50\n",
        "\n",
        "if use_wandb:\n",
        "    wandb.init(project=\"JoJoGAN\")\n",
        "    config = wandb.config\n",
        "    config.num_iter = num_iter\n",
        "    config.preserve_color = preserve_color\n",
        "    wandb.log(\n",
        "    {\"Style reference\": [wandb.Image(transforms.ToPILImage()(target_im))]},\n",
        "    step=0)"
      ],
      "metadata": {
        "id": "MMROuhi1H3zw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load discriminator for perceptual loss."
      ],
      "metadata": {
        "id": "Tnsd-wHUTJ-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = Discriminator(1024, 2).eval().to(device)\n",
        "ckpt = torch.load('models/stylegan2-ffhq-config-f.pt', map_location=lambda storage, loc: storage)\n",
        "discriminator.load_state_dict(ckpt[\"d\"], strict=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdkLZq66ICbu",
        "outputId": "2ea252b3-6cac-40a9-d903-2a018165a947"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=[], unexpected_keys=['final_conv.0.weight', 'final_conv.1.bias', 'final_linear.0.weight', 'final_linear.0.bias', 'final_linear.1.weight', 'final_linear.1.bias'])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reset Generator**"
      ],
      "metadata": {
        "id": "4C3-K3S_TRwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del generator\n",
        "generator = deepcopy(original_generator)\n",
        "g_optim = optim.Adam(generator.parameters(), lr=2e-3, betas=(0, 0.99))"
      ],
      "metadata": {
        "id": "02ydTkuTINSL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Generator**"
      ],
      "metadata": {
        "id": "IVdXrttLTuno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the generator to generated image from the latent space, and optimize the loss."
      ],
      "metadata": {
        "id": "pXBds2dDTxzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if preserve_color:\n",
        "  id_swap = [9,11,15,16,17]\n",
        "\n",
        "  z = range(num_iter)\n",
        "  for idx in tqdm( z):\n",
        "    mean_w = generator.get_latent(torch.randn([latents.size(0), latent_dim]).to(device)).unsqueeze(1).repeat(1, generator.n_latent, 1)\n",
        "\n",
        "    in_latent = latents.clone()\n",
        "\n",
        "    in_latent[:, id_swap] = alpha*latents[:, id_swap] + (1-alpha)*mean_w[:, id_swap]\n",
        "\n",
        "    img = generator(in_latent, input_is_latent=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      real_feat = discriminator(targets)\n",
        "\n",
        "    fake_feat = discriminator(img)\n",
        "\n",
        "    loss = sum([functional.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)\n",
        "\n",
        "    if use_wandb:\n",
        "\n",
        "      wandb.log({\"loss\": loss}, step=idx)\n",
        "\n",
        "      if idx % log_interval == 0:\n",
        "\n",
        "        generator.eval()\n",
        "\n",
        "        my_sample = generator(my_w, input_is_latent=True)\n",
        "\n",
        "        generator.train()\n",
        "\n",
        "        wandb.log(\n",
        "\n",
        "        {\"Current stylization\": [wandb.Image(my_sample)]},\n",
        "\n",
        "        step=idx)\n",
        "\n",
        "    g_optim.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    g_optim.step()"
      ],
      "metadata": {
        "id": "IiAwOs11IRJs"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate JojoGAN Results**"
      ],
      "metadata": {
        "id": "6krDKzYaT3I2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_sample =  5\n",
        "seed = 3000\n",
        "torch.manual_seed(seed)\n",
        "with torch.no_grad():\n",
        "    generator.eval()\n",
        "    z = torch.randn(n_sample, latent_dim, device=device)\n",
        "    original_sample = original_generator([z], truncation=0.7, truncation_latent=mean_latent)\n",
        "    sample = generator([z], truncation=0.7, truncation_latent=mean_latent)\n",
        "    original_my_sample = original_generator(my_w, input_is_latent=True)\n",
        "    my_sample = generator(my_w, input_is_latent=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaYysYx5JE3M",
        "outputId": "6edb5bbc-6849-454e-8c9f-108966d5763b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/JoJoGAN/op/conv2d_gradfix.py:88: UserWarning: conv2d_gradfix not supported on PyTorch 2.1.0+cu121. Falling back to torch.nn.functional.conv2d().\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display Reference Images**"
      ],
      "metadata": {
        "id": "CbkwsJVZUG2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "style_images = []\n",
        "for name in names:\n",
        "    style_path = f'style_images_aligned/{strip_path_extension(name)}.png'\n",
        "    style_image = transform(Image.open(style_path))\n",
        "    style_images.append(style_image)\n",
        "face = transform(aligned_face).to(device).unsqueeze(0)\n",
        "style_images = torch.stack(style_images, 0).to(device)\n",
        "\n",
        "my_output = torch.cat([face, my_sample], 0)\n",
        "output = torch.cat([original_sample, sample], 0)"
      ],
      "metadata": {
        "id": "k0B8zQooJKgq"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}